{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73521821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter no of docs : 4\n",
      "enter path : s1.txt\n",
      "enter path : s2.txt\n",
      "enter path : s3.txt\n",
      "enter path : s4.txt\n",
      "['new home sales tap forecasts new july tap', 'home sales rise in july increase rise july home', 'increase in home sales july home in increase july sales', 'july new home sales rise increase home july tap']\n"
     ]
    }
   ],
   "source": [
    "n=int(input(\"enter no of docs : \"))\n",
    "s=[]\n",
    "for i in range(n):\n",
    "    l=open(input(\"enter path : \")).read()\n",
    "    s.append(l)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764c35eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new home sales tap forecasts new july tap', 'home sales rise in july increase rise july home', 'increase in home sales july home in increase july sales', 'july new home sales rise increase home july tap']\n"
     ]
    }
   ],
   "source": [
    "#removing punctuations\n",
    "punc=\"()-[]{}_!@#$%^&*<>.,;:\"\n",
    "for i in range(n):\n",
    "    for word in s[i]:\n",
    "        if word in punc:\n",
    "            s[i]=s[i].replace(word,\" \")\n",
    "            s[i]=s[i].replace(\"\\n\",\" \")\n",
    "            s[i]=s[i].lower()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1456d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\LAKSHMI\n",
      "[nltk_data]     NARSITHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\LAKSHMI\n",
      "[nltk_data]     NARSITHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f7e642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['new', 'home', 'sales', 'tap', 'forecasts', 'new', 'july', 'tap'], ['home', 'sales', 'rise', 'in', 'july', 'increase', 'rise', 'july', 'home'], ['increase', 'in', 'home', 'sales', 'july', 'home', 'in', 'increase', 'july', 'sales'], ['july', 'new', 'home', 'sales', 'rise', 'increase', 'home', 'july', 'tap']]\n"
     ]
    }
   ],
   "source": [
    "token=[]\n",
    "for i in s:\n",
    "    tk=word_tokenize(i)\n",
    "    token.append(tk)\n",
    "    \n",
    "res=[]\n",
    "for i in token:\n",
    "    for word in i:\n",
    "        if word not in stopwords.words(\"english\") and word not in res:\n",
    "            res.append(word)\n",
    "res.sort()          \n",
    "for k in range(len(s)):\n",
    "    s[k]=s[k].split(\" \")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd1510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forecasts': [1],\n",
       " 'home': [1, 2, 3, 4],\n",
       " 'increase': [2, 3, 4],\n",
       " 'july': [1, 2, 3, 4],\n",
       " 'new': [1, 4],\n",
       " 'rise': [2, 4],\n",
       " 'sales': [1, 2, 3, 4],\n",
       " 'tap': [1, 4]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictx1={}\n",
    "for word in res:\n",
    "    dictx1[word]=[]\n",
    "for word in res:\n",
    "  for i in range(len(s)):\n",
    "    for j in range(len(s[i])):\n",
    "      if word==s[i][j]:\n",
    "        dictx1[word].append(i+1)\n",
    "        break\n",
    "dictx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d26741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectlists(l1,l2):\n",
    "    res=[]\n",
    "    for i in l1:\n",
    "        if i in l2:\n",
    "            res.append(i)\n",
    "    return res\n",
    "def unionlists(l1,l2):\n",
    "    res=[]\n",
    "    for i in l1:\n",
    "        if(i not in res):\n",
    "            res.append(i)\n",
    "    for i in l2:\n",
    "        if(i not in res):\n",
    "            res.append(i)\n",
    "    return res\n",
    "def notop(l1):\n",
    "    res=[]\n",
    "    for i in range(1,len(s)+1):\n",
    "        if(i not in l1):\n",
    "            res.append(i)\n",
    "    return res\n",
    "def findposting(q1):\n",
    "    l1=[]\n",
    "    for i in dictx1:\n",
    "        if i==q1:\n",
    "            l1=dictx1[q1]\n",
    "    return l1\n",
    "\n",
    "def precedence(l1):\n",
    "    if l1==\"not\":\n",
    "        return 1\n",
    "    elif l1==\"and\":\n",
    "        return 2\n",
    "    elif l1==\"or\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e777261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new and tap\n",
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "#boolean queries\n",
    "query=input()\n",
    "query=query.split()\n",
    "stack=[]\n",
    "words=[]\n",
    "idx=-1\n",
    "oper=[\"and\",\"or\",\"not\"]\n",
    "for i in range(len(query)):\n",
    "    if(query[i] not in oper):\n",
    "        words.append(findposting(query[i]))\n",
    "    else:\n",
    "        pre=precedence(query[i])\n",
    "        now=query[i]\n",
    "        if(stack==[]):\n",
    "            stack.append(query[i])\n",
    "            idx=idx+1\n",
    "        elif(precedence(stack[idx])>pre):\n",
    "            stack.append(query[i])\n",
    "            idx=idx+1\n",
    "        elif(precedence(stack[idx])<=pre):\n",
    "            while(stack!=[] and precedence(stack[idx])<pre):\n",
    "                if(stack[idx]==\"and\"):\n",
    "                    l1=words.pop()\n",
    "                    l2=words.pop()\n",
    "                    words.append(intersectlists(l1,l2))\n",
    "                elif(stack[idx]==\"or\"):\n",
    "                    l1=words.pop()\n",
    "                    l2=words.pop()\n",
    "                    words.append(unionlists(l1,l2))\n",
    "                elif(stack[idx]==\"not\"):\n",
    "                    l1=words.pop()\n",
    "                    words.append(notop(l1))\n",
    "                stack.pop()\n",
    "                idx=idx-1\n",
    "            stack.append(now)\n",
    "            idx=idx+1\n",
    "if(stack!=[]):\n",
    "    while(stack!=[]):\n",
    "        if(stack[idx]==\"and\"):\n",
    "            l1=words.pop()\n",
    "            l2=words.pop()\n",
    "            words.append(intersectlists(l1,l2))\n",
    "        elif(stack[idx]==\"or\"):\n",
    "            l1=words.pop()\n",
    "            l2=words.pop()\n",
    "            words.append(unionlists(l1,l2))\n",
    "        elif(stack[idx]==\"not\"):\n",
    "            l1=words.pop()\n",
    "            words.append(notop(l1))\n",
    "        stack.pop()\n",
    "        idx=idx-1\n",
    "    stack.append(now)\n",
    "    idx=idx+1\n",
    "print(sorted(words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a53aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rise new or tap\n",
      "['rise new ', ' tap']\n",
      "{1, 2, 4}\n"
     ]
    }
   ],
   "source": [
    "#phrase queries\n",
    "d={}\n",
    "for word in res:\n",
    "    d[word]=[]\n",
    "for word in res:\n",
    "    for i in range(len(s)):\n",
    "        re={}\n",
    "        l=[]\n",
    "        for j in range(len(s[i])):\n",
    "            if word==s[i][j]:\n",
    "                l.append(i+1)\n",
    "        re[i+1]=l\n",
    "        if(l!=[]):\n",
    "            d[word].append(re)\n",
    "d\n",
    "\n",
    "\n",
    "p=input()\n",
    "q=[\"and\",\"or\",\"not\"]\n",
    "for i in p.split(\" \"):\n",
    "    if i in q:\n",
    "        sam=i\n",
    "        p=p.replace(i,\"\\n\")\n",
    "p=p.split(\"\\n\")\n",
    "print(p)\n",
    "\n",
    "l1=[]\n",
    "for i in p:\n",
    "    i=i.split()\n",
    "    for j in i:\n",
    "        l2=[]\n",
    "        for k in d[j]:\n",
    "            l2.append(int(\" \".join(str(key) for key in k.keys())))\n",
    "        l1.append(l2)\n",
    "if(sam==\"and\"):\n",
    "    print(set(intersectlists(l1[0],l1[1])))\n",
    "elif(sam==\"or\"):\n",
    "    print(set(unionlists(l1[0],l1[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a707cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n",
      "['sky', 'skill', 'skip']\n"
     ]
    }
   ],
   "source": [
    "#wildcard\n",
    "s1=\"rama raja ravi sky skill skip raghava\"\n",
    "sp1=s1.split()\n",
    "g=input()\n",
    "g=g[:len(g)-1]\n",
    "\n",
    "s1=\"$\"+s1+\"$\"\n",
    "t=s1.replace(\" \",\"$\")\n",
    "l=[]\n",
    "for i in range(len(t)-1):\n",
    "    p=t[i]+t[i+1]\n",
    "    l.append(p)\n",
    "\n",
    "k={}\n",
    "for i in l:\n",
    "    m=[]\n",
    "    if \"$\" in i:\n",
    "        if(i[0]==\"$\"):\n",
    "            for j in sp1:\n",
    "                if(i[1] in j):\n",
    "                    m.append(j)\n",
    "            k[i]=m\n",
    "        else:\n",
    "            for j in sp1:\n",
    "                if(l[0] in j):\n",
    "                    m.append(j)\n",
    "            k[i]=m\n",
    "    else:\n",
    "        for j in sp1:\n",
    "            for i in j:\n",
    "                m.append(j)\n",
    "        k[i]=m\n",
    "        \n",
    "l=[]\n",
    "for i in k.keys():\n",
    "    if \"$\" in i:\n",
    "        if(i[0]==\"$\"):\n",
    "            t=i[1]\n",
    "        else:\n",
    "            t=i[0]\n",
    "    else:\n",
    "        t=i\n",
    "    if(t==g):\n",
    "        if(k[i] not in l):\n",
    "            l.append(k[i])\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3e329c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter string1 : haelo\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "#spell correction\n",
    "def min1(a,b,c):\n",
    "  if(a<b and a<c):\n",
    "    return a\n",
    "  elif(b<c):\n",
    "    return b\n",
    "  else:\n",
    "    return c\n",
    "\n",
    "def edit_distance(s1,ma,o):\n",
    "  for i in range(len(ma)):\n",
    "    s1=s1.lower()\n",
    "    s2=ma[i]\n",
    "    s2=s2.lower()\n",
    "    n=len(s1)\n",
    "    t=len(s2)\n",
    "    p=[]\n",
    "    m=[]\n",
    "    for i in range(1):\n",
    "      for j in range(1):\n",
    "        for k in range(t+1):\n",
    "          m.extend([i+k])\n",
    "    p.append(m)\n",
    "    for i in range(1,n+1):\n",
    "      for j in range(1):\n",
    "        l=[]\n",
    "        for k in range(1):\n",
    "          l.extend([i+k])\n",
    "        for k in range(1,t+1):\n",
    "          l.append(0)\n",
    "        p.append(l)\n",
    "    #print(p)\n",
    "    a=list(s1)\n",
    "    b=list(s2)\n",
    "    #print(a)\n",
    "    #print(b)\n",
    "    for i in range(len(a)):\n",
    "      for j in range(len(b)):\n",
    "        if(a[i]==b[j]):\n",
    "            p[i+1][j+1]=min1(p[i+1][j]+1,p[i][j+1]+1,p[i][j]+0)\n",
    "        else:\n",
    "          p[i+1][j+1]=min1(p[i+1][j]+1,p[i][j+1]+1,p[i][j]+1)\n",
    "    mn=p[-1][-1]\n",
    "    o.extend([mn])\n",
    "  low=min(o)\n",
    "  low1=o.index(low)\n",
    "  print(ma[low1])\n",
    "s1=input(\"enter string1 : \")\n",
    "ma = [\"paris\",\"book\",\"hello\"]\n",
    "o=[]\n",
    "edit_distance(s1,ma,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1c14781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'forecasts': [1], 'home': [1, 2], 'increase': [2], 'july': [1, 2], 'new': [1], 'rise': [2], 'sales': [1, 2], 'tap': [1]}, {'home': [3, 4], 'increase': [3, 4], 'july': [3, 4], 'new': [4], 'rise': [4], 'sales': [3, 4], 'tap': [4]}]\n",
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "#bsbi\n",
    "def indexing_block(l1,ind):\n",
    "    s=[]\n",
    "    for i in range(len(l1)):\n",
    "        l=open(l1[i]).read()\n",
    "        s.append(l)\n",
    "    punc=\"()-[]{}_!@#$%^&*<>.,;:\"\n",
    "    for i in range(len(s)):\n",
    "        for word in s[i]:\n",
    "            if word in punc:\n",
    "                s[i]=s[i].replace(word,\" \")\n",
    "                s[i]=s[i].replace(\"\\n\",\" \")\n",
    "                s[i]=s[i].lower()\n",
    "    \n",
    "    token=[]\n",
    "    for i in s:\n",
    "        tk=word_tokenize(i)\n",
    "        token.append(tk)\n",
    "\n",
    "    res=[]\n",
    "    for i in token:\n",
    "        for word in i:\n",
    "            if word not in stopwords.words(\"english\") and word not in res:\n",
    "                res.append(word)\n",
    "    res.sort()          \n",
    "    for k in range(len(s)):\n",
    "        s[k]=s[k].split(\" \")\n",
    "        \n",
    "        \n",
    "    dictx1={} \n",
    "    for word in res:\n",
    "        if(word not in dictx1.keys()):\n",
    "            dictx1[word]=[]\n",
    "    for word in res:\n",
    "      for i in range(len(s)):\n",
    "        for j in range(len(s[i])):\n",
    "          if word==s[i][j]:\n",
    "            dictx1[word].append(i+1+ind)\n",
    "            break\n",
    "    return dictx1\n",
    "\n",
    "m1=[\"s1.txt\",\"s2.txt\"]\n",
    "m2=[\"s3.txt\",\"s4.txt\"]\n",
    "dictx1={}\n",
    "l=[]\n",
    "l.append(indexing_block(m1,0))\n",
    "l.append(indexing_block(m2,2))\n",
    "print(l)\n",
    "res_key=[]\n",
    "for i in l:\n",
    "    res_key.extend(i.keys())\n",
    "res_key.sort()\n",
    "res_keys=[]\n",
    "[res_keys.append(x) for x in res_key if x not in res_keys]\n",
    "res_dict={}\n",
    "for i in res_keys:\n",
    "    result=[]\n",
    "    for dix in l:\n",
    "        if i in dix.keys():\n",
    "            result.extend(dix[i])\n",
    "        res_dict=result\n",
    "print(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5151e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forecasts': [1], 'home': [1, 2, 3, 4], 'increase': [2, 3, 4], 'july': [1, 2, 3, 4], 'new': [1, 4], 'rise': [2, 4], 'sales': [1, 2, 3, 4], 'tap': [1, 4]}\n"
     ]
    }
   ],
   "source": [
    "#spimi\n",
    "def indexing_block(dictx1,l1,ind):\n",
    "    s=[]\n",
    "    for i in range(len(l1)):\n",
    "        l=open(l1[i]).read()\n",
    "        s.append(l)\n",
    "    punc=\"()-[]{}_!@#$%^&*<>.,;:\"\n",
    "    for i in range(len(s)):\n",
    "        for word in s[i]:\n",
    "            if word in punc:\n",
    "                s[i]=s[i].replace(word,\" \")\n",
    "                s[i]=s[i].replace(\"\\n\",\" \")\n",
    "                s[i]=s[i].lower()\n",
    "    \n",
    "    token=[]\n",
    "    for i in s:\n",
    "        tk=word_tokenize(i)\n",
    "        token.append(tk)\n",
    "\n",
    "    res=[]\n",
    "    for i in token:\n",
    "        for word in i:\n",
    "            if word not in stopwords.words(\"english\") and word not in res:\n",
    "                res.append(word)\n",
    "    res.sort()          \n",
    "    for k in range(len(s)):\n",
    "        s[k]=s[k].split(\" \")\n",
    "    \n",
    "    for word in res:\n",
    "        if(word not in dictx1.keys()):\n",
    "            dictx1[word]=[]\n",
    "    for word in res:\n",
    "      for i in range(len(s)):\n",
    "        for j in range(len(s[i])):\n",
    "          if word==s[i][j]:\n",
    "            dictx1[word].append(i+1+ind)\n",
    "            break\n",
    "    return dictx1\n",
    "\n",
    "m1=[\"s1.txt\",\"s2.txt\"]\n",
    "m2=[\"s3.txt\",\"s4.txt\"]\n",
    "dictx1={}\n",
    "l=[]\n",
    "l.append(indexing_block(dictx1,m1,0))\n",
    "l.append(indexing_block(dictx1,m2,2))\n",
    "print(l[len(l)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da3a6b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['new', 'home', 'sales', 'tap', 'forecasts', 'new', 'july', 'tap'], ['home', 'sales', 'rise', 'in', 'july', 'increase', 'rise', 'july', 'home']]\n",
      "{'rise', 'tap', 'in', 'sales', 'july', 'new', 'increase', 'forecasts', 'home'}\n",
      "{0: [0.0, 1.3862943611198906, 0.0, 0.0, 0.0, 1.3862943611198906, 0.0, 0.6931471805599453, 0.0], 1: [1.3862943611198906, 0.0, 0.6931471805599453, 0.0, 0.0, 0.0, 0.6931471805599453, 0.0, 0.0]}\n",
      "new home\n",
      "[0.4714045207910317, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "l=[\"s1.txt\",'s2.txt']\n",
    "s=[]\n",
    "for i in range(2):\n",
    "  f=open(l[i],encoding=\"UTF-8\")\n",
    "  read=f.read()\n",
    "  s.append(read)\n",
    "  f.seek(0)\n",
    "#counting number of lines\n",
    "line=1\n",
    "for i in s:\n",
    "  for word in i:\n",
    "    if word=='\\n':\n",
    "      line+=1\n",
    "#removing punctuations and new lines\n",
    "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "for i in range(len(s)):\n",
    "  for word in s[i]:\n",
    "    if word in punc:\n",
    "      s[i]=s[i].replace(word,\" \")\n",
    "      s[i]=s[i].replace('\\n',\" \")\n",
    "      s[i]=s[i].lower()\n",
    "for k in range(len(s)):\n",
    "  s[k]=s[k].split()\n",
    "print(s)\n",
    "\n",
    "vocab=[]\n",
    "for i in s:\n",
    "    vocab.extend(i)\n",
    "vocab=set(vocab)\n",
    "print(vocab)\n",
    "\n",
    "mat={}\n",
    "for i in range(len(s)):\n",
    "    res=[]\n",
    "    for word in vocab:\n",
    "        tf=s[i].count(word)\n",
    "        df=0\n",
    "        for k in s:\n",
    "            if word in k:\n",
    "                df=df+1\n",
    "        idf=math.log(2/df)\n",
    "        res.append(tf*idf)\n",
    "    mat[i]=res\n",
    "print(mat)\n",
    "query=input()\n",
    "queryvec=[]\n",
    "query=query.split(\" \")\n",
    "querysum=0\n",
    "for i in vocab:\n",
    "  if i in query:\n",
    "    queryvec.append(1)\n",
    "    querysum+=1\n",
    "  else:\n",
    "    queryvec.append(0)\n",
    "    querysum+=0\n",
    "querymag=math.sqrt(querysum)\n",
    "cosine=[]\n",
    "for key in mat.keys():\n",
    "  temp=np.dot(queryvec,mat[key])\n",
    "  sum=0\n",
    "  for i in mat[key]:\n",
    "    sum+=(i**2)\n",
    "  mag=math.sqrt(sum)\n",
    "  cosine.append(temp/(mag*querymag))\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89202c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00030121377997263036\n",
      "0.00013548070246744226\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "cdoc=[\"test1.txt\",\"test2.txt\",\"text3.txt\"]\n",
    "cbardoc=[\"test4.txt\"]\n",
    "f=open(\"test5.txt\")\n",
    "totaldocs=len(cdoc)+len(cbardoc)\n",
    "read=f.read()\n",
    "query=str(read)\n",
    "\n",
    "c=[]\n",
    "for i in cdoc:\n",
    "    f=open(i)\n",
    "    read=f.read()\n",
    "    c.extend(read.split())\n",
    "    \n",
    "cbar=[]\n",
    "for i in cbardoc:\n",
    "    f=open(i)\n",
    "    read=f.read()\n",
    "    cbar.extend(read.split())\n",
    "vocab=c+cbar\n",
    "vocab=set(vocab)\n",
    "pyes=1\n",
    "pno=1\n",
    "query=query.split(\" \")\n",
    "for i in query:\n",
    "    pyes*=(c.count(i)+1)/(len(c)+len(vocab))\n",
    "for i in query:\n",
    "    pno*=(cbar.count(i)+1)/(len(cbar)+len(vocab))\n",
    "\n",
    "pyes=(len(cdoc)/totaldocs)*pyes\n",
    "pno=(len(cbardoc)/totaldocs)*pno\n",
    "print(pyes)\n",
    "print(pno)\n",
    "if(pyes>pno):\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dad07fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "cat1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#k-nn\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "iris=datasets.load_iris()\n",
    "iris\n",
    "q=[2.3,3.4]\n",
    "k=15\n",
    "x=iris.data[:,:2]\n",
    "y=iris.target\n",
    "def distance(x,q):\n",
    "    a=[]\n",
    "    for i in range(len(x)):\n",
    "        a1=(x[i][0]-q[0])**2\n",
    "        a2=(x[i][1]-q[1])**2\n",
    "        a.append(math.sqrt(a1+a2))\n",
    "    return a\n",
    "c=distance(x,q)\n",
    "idx=np.argsort(c)\n",
    "t=y[idx][:k]\n",
    "t=list(t)\n",
    "print(t)\n",
    "zero=t.count(0)\n",
    "one=t.count(1)\n",
    "two=t.count(2)\n",
    "if(zero>one and zero>two):\n",
    "    m=\"cat1\"\n",
    "elif(one>two):\n",
    "    m=\"cat2\"\n",
    "else:\n",
    "    m=\"cat3\"\n",
    "print(m)\n",
    "\n",
    "col=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n",
    "df=pd.DataFrame(iris.data,columns=col)\n",
    "plt.plot(df[\"sepal_length\"][:50],df[\"sepal_width\"][:50],\"b.\")\n",
    "plt.plot(df[\"sepal_length\"][50:100],df[\"sepal_width\"][50:100],\"g.\")\n",
    "plt.plot(df[\"sepal_length\"][100:],df[\"sepal_width\"][100:],\"r*\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec24bd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter no of clusters : 6\n",
      "1  :  [1]\n",
      "2  :  [2]\n",
      "3  :  [3, 7]\n",
      "4  :  [11, 8, 9, 10]\n",
      "5  :  [51]\n",
      "6  :  [69]\n"
     ]
    }
   ],
   "source": [
    "#k-means\n",
    "def mean(s):\n",
    "    return sum(s)/len(s)\n",
    "l=[1,2,3,11,51,69,7,8,9,10]\n",
    "n=int(input(\"enter no of clusters : \"))\n",
    "clusters={}\n",
    "for i in range(n):\n",
    "    clusters[i+1]=[[],l[i]]\n",
    "\n",
    "while True:\n",
    "    present={}\n",
    "    for cluster in clusters:\n",
    "        present[cluster]=[]\n",
    "    for doc in l:\n",
    "        m=1000\n",
    "        c=\" \"\n",
    "        for cluster in clusters:\n",
    "            if(m>abs(doc-clusters[cluster][-1])):\n",
    "                m=abs(doc-clusters[cluster][-1])\n",
    "                c=cluster\n",
    "        present[c].append(doc)\n",
    "    if(present[c]==clusters[c][0]):\n",
    "        break\n",
    "    else:\n",
    "        for cluster in clusters:\n",
    "            clusters[cluster][0]=present[cluster]\n",
    "            clusters[cluster][1]=mean(present[cluster])\n",
    "for cluster in clusters:\n",
    "    print(cluster,\" : \",clusters[cluster][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
